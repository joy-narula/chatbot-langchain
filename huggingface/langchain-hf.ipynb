{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|██████████| 2/2 [01:48<00:00, 54.43s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 22.26s/it]\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id = \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    task = \"text-generation\",\n",
    "    pipeline_kwargs = {\"temperature\": 0.1, \n",
    "                       \"max_new_tokens\": 100,\n",
    "                       \"top_k\":50\n",
    "                       }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Langchain is all about creating a seamless and efficient workflow for data scientists and developers. It provides a comprehensive set of tools and libraries that enable users to easily build, train, and deploy machine learning models.\\n\\n### Key Features of Chain\\n\\n- **Chain**: The core component of Langchain, Chain is a powerful tool for building and executing machine learning workflows. It provides a flexible and extensible framework for defining and executing complex data processing pipelines.\\n\\n-'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"Langchain is all about\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" a popular open-source project that provides pre-trained models for natural language processing (NLP) and other AI applications. One of the most popular models is BERT (Bidirectional Encoder Representations from Transformers), which has achieved state-of-the-art results in many NLP tasks, such as question-answering, sentiment analysis, and language translation.\\n\\nIn this article, we'll explore how to use the Hugging Face Transformers library to load and fine-tune a pre-trained BERT model for a\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id = \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    task = \"text-generation\",\n",
    "    max_new_tokens = 100,\n",
    "    do_sample=False,\n",
    ")\n",
    "\n",
    "llm.invoke(\"Hugging Face is\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
